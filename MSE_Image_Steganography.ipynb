{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSE_Image_Steganography.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eereenah/deep-steganography/blob/master/MSE_Image_Steganography.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud98xN3LiRbR",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocZ7PT68hHcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import socket\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.utils as vutils\n",
        "#from tensorboardX import SummaryWriter\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_8PdKP_HgQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP-i5m3FiQYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "from PIL import Image, ImageOps, ImageEnhance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzYJBhGOiYBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    import accimage\n",
        "except ImportError:\n",
        "    accimage = None\n",
        "import numpy as np\n",
        "import numbers\n",
        "import types\n",
        "import collections\n",
        "import warnings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV3AaFtx-Y9F",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiiTms2-8E4x",
        "colab_type": "code",
        "outputId": "b2bdf18f-93b4-4655-c97c-1290610aea22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "! wget images.cocodataset.org/zips/val2017.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-21 18:50:38--  http://images.cocodataset.org/zips/val2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.205.35\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.205.35|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 815585330 (778M) [application/zip]\n",
            "Saving to: ‘val2017.zip’\n",
            "\n",
            "val2017.zip         100%[===================>] 777.80M  67.0MB/s    in 12s     \n",
            "\n",
            "2020-01-21 18:50:50 (65.8 MB/s) - ‘val2017.zip’ saved [815585330/815585330]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj3DsJX18rLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip -qq *.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZWnAO4LbHnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm *.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CszwA5e9stZ",
        "colab_type": "code",
        "outputId": "ee2351aa-81ab-401e-da6f-1447218e2169",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "! wget images.cocodataset.org/zips/test2017.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-01-21 18:51:03--  http://images.cocodataset.org/zips/test2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.217.10.92\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.217.10.92|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6646970404 (6.2G) [application/zip]\n",
            "Saving to: ‘test2017.zip’\n",
            "\n",
            "test2017.zip        100%[===================>]   6.19G  61.3MB/s    in 1m 56s  \n",
            "\n",
            "2020-01-21 18:52:59 (54.7 MB/s) - ‘test2017.zip’ saved [6646970404/6646970404]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtV6MhU8bNDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip -qq *.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIFu7aJEbPsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm *.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhdYejDEl_QG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir 'val2017/cover'\n",
        "! mkdir 'val2017/secret'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD-A1c7Fm0CC",
        "colab_type": "code",
        "outputId": "12322eaf-5919-41e5-d7df-89c7e7385c5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "! find 'val2017/' -maxdepth 1 -type f -printf \".\" | wc -c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1E2ptSJoaa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls 'val2017/'* | head -480 | xargs -I{} mv {} 'val2017/cover/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd9K0c6Cw3cJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls 'val2017/'* | head -480| xargs -I{} mv {} 'val2017/secret/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M47zTd_Nbcbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir 'train2017'\n",
        "! mkdir 'train2017/cover'\n",
        "! mkdir 'train2017/secret'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G-wpcMdbikg",
        "colab_type": "code",
        "outputId": "66ed4bd1-bab8-444a-c5f4-90fe69f3a985",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "! find 'test2017/' -maxdepth 1 -type f -printf \".\" | wc -c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "40670\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0ye6oHtbm_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls 'test2017/'* | head -4800 | xargs -I{} mv {} 'train2017/cover/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjScNIQFbqST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls 'test2017/'* | head -4800 | xargs -I{} mv {} 'train2017/secret/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsiwGz67byRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir 'test2017/cover'\n",
        "! mkdir 'test2017/secret'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBHqBPonY8ca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls 'test2017/'* | head -480 | xargs -I{} mv {} 'test2017/cover/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkB2xLQKZBHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls 'test2017/'* | head -4800 | xargs -I{} mv {} 'test2017/secret/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t60A1y5Ww4Dq",
        "colab_type": "code",
        "outputId": "8ecb237f-8c07-468b-e3fb-81d43de9a0b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzisbWqoxFIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4dqDq5qh2Qw",
        "colab_type": "text"
      },
      "source": [
        "### Hiding Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phYnFeB9hoLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UnetGenerator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n",
        "                 norm_layer=nn.BatchNorm2d, use_dropout=False, output_function=nn.Sigmoid):\n",
        "        super(UnetGenerator, self).__init__()\n",
        "        # construct unet structure\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\n",
        "        for i in range(num_downs - 5):\n",
        "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer, output_function=output_function)\n",
        "\n",
        "        self.model = unet_block\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YePxcnYKhw8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UnetSkipConnectionBlock(nn.Module):\n",
        "    def __init__(self, outer_nc, inner_nc, input_nc=None,submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False, output_function=nn.Sigmoid):\n",
        "        super(UnetSkipConnectionBlock, self).__init__()\n",
        "        self.outermost = outermost\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        if input_nc is None:\n",
        "            input_nc = outer_nc\n",
        "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
        "                             stride=2, padding=1, bias=use_bias)\n",
        "        downrelu = nn.LeakyReLU(0.2, True)\n",
        "        downnorm = norm_layer(inner_nc)\n",
        "        uprelu = nn.ReLU(True)\n",
        "        upnorm = norm_layer(outer_nc)\n",
        "\n",
        "        if outermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1)\n",
        "            down = [downconv]\n",
        "            if output_function == nn.Tanh:\n",
        "                up = [uprelu, upconv, nn.Tanh()]\n",
        "            else:\n",
        "                up = [uprelu, upconv, nn.Sigmoid()]\n",
        "            model = down + [submodule] + up\n",
        "        elif innermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1, bias=use_bias)\n",
        "            down = [downrelu, downconv]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            model = down + up\n",
        "        else:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1, bias=use_bias)\n",
        "            down = [downrelu, downconv, downnorm]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "\n",
        "            if use_dropout:\n",
        "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
        "            else:\n",
        "                model = down + [submodule] + up\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.outermost:\n",
        "            return self.model(x)\n",
        "        else:\n",
        "            return torch.cat([x, self.model(x)], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjO163cUh-BY",
        "colab_type": "text"
      },
      "source": [
        "### Revealing Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxZ-MJlDhzsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RevealNet(nn.Module):\n",
        "    def __init__(self, nc=3, nhf=64, output_function=nn.Sigmoid):\n",
        "        super(RevealNet, self).__init__()\n",
        "        # input is (3) x 256 x 256\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(nc, nhf, 3, 1, 1),\n",
        "            nn.BatchNorm2d(nhf),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(nhf, nhf * 2, 3, 1, 1),\n",
        "            nn.BatchNorm2d(nhf*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(nhf * 2, nhf * 4, 3, 1, 1),\n",
        "            nn.BatchNorm2d(nhf*4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(nhf * 4, nhf * 2, 3, 1, 1),\n",
        "            nn.BatchNorm2d(nhf*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(nhf * 2, nhf, 3, 1, 1),\n",
        "            nn.BatchNorm2d(nhf),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(nhf, nc, 3, 1, 1),\n",
        "            output_function()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output=self.main(input)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgGI49wcj3Sf",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka9xm2VgU_-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_beta = 0.75\n",
        "opt_niter = 10\n",
        "opt_logFreq = 10\n",
        "opt_saveFreq = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvvYvP4Sl6-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQk6it-5mPNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, epoch, Hnet, Rnet, criterion):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    Hlosses = AverageMeter()  # record loss of H-net\n",
        "    Rlosses = AverageMeter()  # record loss of R-net\n",
        "    SumLosses = AverageMeter()  # record Hloss + β*Rloss\n",
        "\n",
        "    # switch to train mode\n",
        "    Hnet.train()\n",
        "    Rnet.train()\n",
        "\n",
        "    start_time = time.time()\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        data_time.update(time.time() - start_time)\n",
        "\n",
        "        Hnet.zero_grad()\n",
        "        Rnet.zero_grad()\n",
        "\n",
        "        all_pics = data[0]  # allpics contains cover images and secret images\n",
        "        this_batch_size = int(all_pics.size()[0] / 2)  # get true batch size of this step \n",
        "\n",
        "        # first half of images will become cover images, the rest are treated as secret images\n",
        "        cover_img = all_pics[0:this_batch_size, :, :, :]  # batchsize,3,256,256\n",
        "        secret_img = all_pics[this_batch_size:this_batch_size * 2, :, :, :]\n",
        "\n",
        "        # concat cover images and secret images as input of H-net\n",
        "        concat_img = torch.cat([cover_img, secret_img], dim=1)\n",
        "\n",
        "        concat_imgv = Variable(concat_img)\n",
        "        cover_imgv = Variable(cover_img)\n",
        "\n",
        "        container_img = Hnet(concat_imgv)  # put concat_image into H-net and get container image\n",
        "        errH = criterion(container_img, cover_imgv)  # loss between cover and container\n",
        "        Hlosses.update(errH.data.item(), this_batch_size)\n",
        "\n",
        "        rev_secret_img = Rnet(container_img)  # put concatenated image into R-net and get revealed secret image\n",
        "        secret_imgv = Variable(secret_img)\n",
        "        errR = criterion(rev_secret_img, secret_imgv)  # loss between secret image and revealed secret image\n",
        "        Rlosses.update(errR.data.item(), this_batch_size)\n",
        "\n",
        "        betaerrR_secret = opt_beta * errR\n",
        "        err_sum = errH + betaerrR_secret\n",
        "        SumLosses.update(err_sum.data.item(), this_batch_size)\n",
        "\n",
        "        err_sum.backward()\n",
        "\n",
        "        optimizerH.step()\n",
        "        optimizerR.step()\n",
        "\n",
        "        batch_time.update(time.time() - start_time)\n",
        "        start_time = time.time()\n",
        "\n",
        "        log = '[%d/%d][%d/%d]\\tLoss_H: %.4f Loss_R: %.4f Loss_sum: %.4f \\tdatatime: %.4f \\tbatchtime: %.4f' % (\n",
        "            epoch, opt_niter, i, len(train_loader),\n",
        "            Hlosses.val, Rlosses.val, SumLosses.val, data_time.val, batch_time.val)\n",
        "\n",
        "        if i % opt_logFreq == 0:\n",
        "            print(log)\n",
        "            with open('drive/My Drive/Steganography/Models/UNet-Pytorch/Checkpoints/Checkpoints_4800_10/Parameters.txt', 'a') as f: \n",
        "                f.write(log + \"\\n\")\n",
        "\n",
        "\n",
        "        if epoch % 1 == 0 and i % opt_saveFreq == 0:\n",
        "            save_result_pic(this_batch_size, cover_img, container_img.data, secret_img, rev_secret_img.data, epoch, i,\n",
        "                            'drive/My Drive/Steganography/Models/UNet-Pytorch/Train/Train_4800_10_p2/')\n",
        "\n",
        "\n",
        "    # epcoh log\n",
        "    epoch_log = \"one epoch time is %.4f======================================================================\" % (\n",
        "        batch_time.sum) + \"\\n\"\n",
        "    epoch_log = epoch_log + \"epoch learning rate: optimizerH_lr = %.8f      optimizerR_lr = %.8f\" % (\n",
        "        optimizerH.param_groups[0]['lr'], optimizerR.param_groups[0]['lr']) + \"\\n\"\n",
        "    epoch_log = epoch_log + \"epoch_Hloss=%.6f\\tepoch_Rloss=%.6f\\tepoch_sumLoss=%.6f\" % (\n",
        "        Hlosses.avg, Rlosses.avg, SumLosses.avg)\n",
        "    print(epoch_log)\n",
        "    with open('drive/My Drive/Steganography/Models/UNet-Pytorch/Checkpoints/Checkpoints_4800_10/Parameters.txt', 'a') as f: \n",
        "                f.write(epoch_log + \"\\n\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qjgQnhCmVlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation(val_loader, epoch, Hnet, Rnet, criterion):\n",
        "    print(\n",
        "        \"#################################################### validation begin ########################################################\")\n",
        "    start_time = time.time()\n",
        "    Hnet.eval()\n",
        "    Rnet.eval()\n",
        "    Hlosses = AverageMeter()\n",
        "    Rlosses = AverageMeter()\n",
        "    for i, data in enumerate(val_loader, 0):\n",
        "        Hnet.zero_grad()\n",
        "        Rnet.zero_grad()\n",
        "        all_pics = data[0]\n",
        "        this_batch_size = int(all_pics.size()[0] / 2)\n",
        "\n",
        "        cover_img = all_pics[0:this_batch_size, :, :, :]\n",
        "        secret_img = all_pics[this_batch_size:this_batch_size * 2, :, :, :]\n",
        "\n",
        "        concat_img = torch.cat([cover_img, secret_img], dim=1)\n",
        "\n",
        "\n",
        "        concat_imgv = Variable(concat_img, volatile=True)\n",
        "        cover_imgv = Variable(cover_img, volatile=True)\n",
        "\n",
        "        container_img = Hnet(concat_imgv)\n",
        "        errH = criterion(container_img, cover_imgv)\n",
        "        Hlosses.update(errH.data.item(), this_batch_size)\n",
        "\n",
        "        rev_secret_img = Rnet(container_img)\n",
        "        secret_imgv = Variable(secret_img, volatile=True)\n",
        "        errR = criterion(rev_secret_img, secret_imgv)\n",
        "        Rlosses.update(errR.data.item(), this_batch_size)\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            save_result_pic(this_batch_size, cover_img, container_img.data, secret_img, rev_secret_img.data, epoch, i,\n",
        "                            'drive/My Drive/Steganography/Models/UNet-Pytorch/Validation/Validation_4800_10_p2/')\n",
        "\n",
        "\n",
        "    val_hloss = Hlosses.avg\n",
        "    val_rloss = Rlosses.avg\n",
        "    val_sumloss = val_hloss + opt_beta * val_rloss\n",
        "\n",
        "    val_time = time.time() - start_time\n",
        "    val_log = \"validation[%d] val_Hloss = %.6f\\t val_Rloss = %.6f\\t val_Sumloss = %.6f\\t validation time=%.2f\" % (\n",
        "        epoch, val_hloss, val_rloss, val_sumloss, val_time)\n",
        "    #print_log(val_log, logPath)\n",
        "    print(val_log)\n",
        "    with open('drive/My Drive/Steganography/Models/UNet-Pytorch/Checkpoints/Checkpoints_4800_10/Parameters.txt', 'a') as f: \n",
        "                f.write(val_log + \"\\n\")\n",
        "\n",
        "\n",
        "    print(\n",
        "        \"#################################################### validation end ########################################################\")\n",
        "    return val_hloss, val_rloss, val_sumloss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMeBCZpVmar9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(test_loader, epoch, Hnet, Rnet, criterion):\n",
        "    print(\n",
        "        \"#################################################### test begin ########################################################\")\n",
        "    start_time = time.time()\n",
        "    Hnet.eval()\n",
        "    Rnet.eval()\n",
        "    Hlosses = AverageMeter()  # record the Hloss in one epoch\n",
        "    Rlosses = AverageMeter()  # record the Rloss in one epoch\n",
        "    for i, data in enumerate(test_loader, 0):\n",
        "        Hnet.zero_grad()\n",
        "        Rnet.zero_grad()\n",
        "        all_pics = data[0]  # allpics contains cover images and secret images\n",
        "        this_batch_size = int(all_pics.size()[0] / 2)  # get true batch size of this step \n",
        "\n",
        "        # first half of images will become cover images, the rest are treated as secret images\n",
        "        cover_img = all_pics[0:this_batch_size, :, :, :]  # batchSize,3,256,256\n",
        "        secret_img = all_pics[this_batch_size:this_batch_size * 2, :, :, :]\n",
        "\n",
        "        # concat cover and original secret to get the concat_img with 6 channels\n",
        "        concat_img = torch.cat([cover_img, secret_img], dim=1)\n",
        "\n",
        "\n",
        "        concat_imgv = Variable(concat_img, volatile=True)  # concat_img as input of Hiding net\n",
        "        cover_imgv = Variable(cover_img, volatile=True)  # cover_imgv as label of Hiding net\n",
        "\n",
        "        container_img = Hnet(concat_imgv)  # take concat_img as input of H-net and get the container_img\n",
        "        errH = criterion(container_img, cover_imgv)  # H-net reconstructed error\n",
        "        Hlosses.update(errH.data.item(), this_batch_size)\n",
        "\n",
        "        rev_secret_img = Rnet(container_img)  # containerImg as input of R-net and get \"rev_secret_img\"\n",
        "        secret_imgv = Variable(secret_img, volatile=True)  # secret_imgv as label of R-net\n",
        "        errR = criterion(rev_secret_img, secret_imgv)  # R-net reconstructed error\n",
        "        Rlosses.update(errR.data.item(), this_batch_size)\n",
        "        save_result_pic(this_batch_size, cover_img, container_img.data, secret_img, rev_secret_img.data, epoch, i,\n",
        "                            'drive/My Drive/Steganography/Models/UNet-Pytorch/Test/Test_4800_10/')\n",
        "        # save_result_pic(this_batch_size, cover_img, container_img.data, secret_img, rev_secret_img.data, epoch, i,\n",
        "        #                 opt.testPics)\n",
        "\n",
        "    val_hloss = Hlosses.avg\n",
        "    val_rloss = Rlosses.avg\n",
        "    val_sumloss = val_hloss + opt_beta * val_rloss\n",
        "\n",
        "    val_time = time.time() - start_time\n",
        "    val_log = \"validation[%d] val_Hloss = %.6f\\t val_Rloss = %.6f\\t val_Sumloss = %.6f\\t validation time=%.2f\" % (\n",
        "        epoch, val_hloss, val_rloss, val_sumloss, val_time)\n",
        "    #print_log(val_log, logPath)\n",
        "    print(val_log)\n",
        "    with open('drive/My Drive/Steganography/Models/UNet-Pytorch/Checkpoints/Checkpoints_4800_10/Parameters.txt', 'a') as f: \n",
        "                f.write(val_log + \"\\n\")\n",
        "\n",
        "\n",
        "    print(\n",
        "        \"#################################################### test end ########################################################\")\n",
        "    return val_hloss, val_rloss, val_sumloss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSvZbCZYme3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_result_pic(this_batch_size, originalLabelv, ContainerImg, secretLabelv, RevSecImg, epoch, i, save_path):\n",
        "      originalFrames = originalLabelv.resize_(this_batch_size, 3, 256, 256)\n",
        "      containerFrames = ContainerImg.resize_(this_batch_size, 3, 256, 256)\n",
        "      secretFrames = secretLabelv.resize_(this_batch_size, 3, 256, 256)\n",
        "      revSecFrames = RevSecImg.resize_(this_batch_size, 3, 256, 256)\n",
        "\n",
        "      showContainer = torch.cat([originalFrames, containerFrames], 0)\n",
        "      showReveal = torch.cat([secretFrames, revSecFrames], 0)\n",
        "      resultImg = torch.cat([showContainer, showReveal], 0)\n",
        "      resultImgName = '%s/ResultPics_epoch%03d_batch%04d.png' % (save_path, epoch, i)\n",
        "      vutils.save_image(resultImg, resultImgName, nrow=this_batch_size, padding=1, normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw7hp6m8mhSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPNlq18CmHwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    #global parameters\n",
        "    global opt, optimizerH, optimizerR, writer, logPath, schedulerH, schedulerR, val_loader, smallestLoss\n",
        "\n",
        "    Hnet = UnetGenerator(input_nc=6, output_nc=3, num_downs=7, output_function=nn.Sigmoid)\n",
        "    Hnet.apply(weights_init)\n",
        "\n",
        "    Rnet = RevealNet(output_function=nn.Sigmoid)\n",
        "    Rnet.apply(weights_init)\n",
        "\n",
        "    Hnet.load_state_dict(torch.load(\"drive/My Drive/Steganography/Models/UNet-Pytorch/Checkpoints/Checkpoints_4800_10/netH_epoch_5,sumloss=0.006057,Hloss=0.002503.pth\"))\n",
        "    Rnet.load_state_dict(torch.load(\"drive/My Drive/Steganography/Models/UNet-Pytorch/Checkpoints/Checkpoints_4800_10/netR_epoch_5,sumloss=0.006057,Rloss=0.004739.pth\"))\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizerH = optim.Adam(Hnet.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
        "    schedulerH = ReduceLROnPlateau(optimizerH, mode='min', factor=0.2, patience=5, verbose=True)\n",
        "\n",
        "    optimizerR = optim.Adam(Rnet.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
        "    schedulerR = ReduceLROnPlateau(optimizerR, mode='min', factor=0.2, patience=8, verbose=True)\n",
        "\n",
        "    train_list = torchvision.datasets.ImageFolder(\n",
        "        root='train2017/',\n",
        "        transform= torchvision.transforms.Compose([                                                      \n",
        "                                       torchvision.transforms.Resize([256, 256]),\n",
        "                                       torchvision.transforms.ToTensor()\n",
        "       ])\n",
        "    )\n",
        "\n",
        "\n",
        "    val_list = torchvision.datasets.ImageFolder(\n",
        "       root='val2017/',\n",
        "       transform= torchvision.transforms.Compose([                                                      \n",
        "                                       torchvision.transforms.Resize([256, 256]),\n",
        "                                       torchvision.transforms.ToTensor()\n",
        "       ])\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_list, batch_size=32,\n",
        "                              shuffle=True, num_workers=int(8))\n",
        "    val_loader = DataLoader(val_list, batch_size=2,\n",
        "                            shuffle=False, num_workers=int(8))\n",
        "    smallestLoss = 10000\n",
        "    for epoch in range(opt_niter):\n",
        "        #train\n",
        "        train(train_loader, epoch, Hnet=Hnet, Rnet=Rnet, criterion=criterion)\n",
        "\n",
        "        #validation\n",
        "        val_hloss, val_rloss, val_sumloss = validation(val_loader, epoch, Hnet=Hnet, Rnet=Rnet, criterion=criterion)\n",
        "\n",
        "        #learning rate\n",
        "        schedulerH.step(val_sumloss)\n",
        "        schedulerR.step(val_rloss)\n",
        "\n",
        "        #save model\n",
        "        if val_sumloss < globals()[\"smallestLoss\"]:\n",
        "            globals()[\"smallestLoss\"] = val_sumloss\n",
        "            torch.save(Hnet.state_dict(),\n",
        "                        '%s/netH_epoch_%d,sumloss=%.6f,Hloss=%.6f.pth' % (\n",
        "                            'drive/My Drive/Steganography/Models/UNet-Pytorch/Checkpoints/Checkpoints_4800_10/', epoch, val_sumloss, val_hloss))\n",
        "            torch.save(Rnet.state_dict(),\n",
        "                        '%s/netR_epoch_%d,sumloss=%.6f,Rloss=%.6f.pth' % (\n",
        "                            'drive/My Drive/Steganography/Models/UNet-Pytorch/Checkpoints/Checkpoints_4800_10/', epoch, val_sumloss, val_rloss))\n",
        "            \n",
        "    test_list = torchvision.datasets.ImageFolder(\n",
        "        root='test2017/',\n",
        "        transform= torchvision.transforms.Compose([                                                      \n",
        "                                        torchvision.transforms.Resize([256, 256]),\n",
        "                                        torchvision.transforms.ToTensor()\n",
        "        ])\n",
        "    )\n",
        "\n",
        "\n",
        "    test_loader = DataLoader(test_list, batch_size=32,\n",
        "                                  shuffle=False, num_workers=int(8))\n",
        "    test(test_loader, 0, Hnet=Hnet, Rnet=Rnet, criterion=criterion)\n",
        "    print(\"##################   test is completed, the result pic is saved in the ./training/yourcompuer+time/testPics/   ######################\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lHlW3x7mrEW",
        "colab_type": "code",
        "outputId": "982547ac-1f6a-4852-f8d9-67b584684365",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0/10][0/300]\tLoss_H: 0.0026 Loss_R: 0.0038 Loss_sum: 0.0055 \tdatatime: 0.4906 \tbatchtime: 13.9473\n",
            "[0/10][10/300]\tLoss_H: 0.0032 Loss_R: 0.0029 Loss_sum: 0.0054 \tdatatime: 0.0002 \tbatchtime: 9.8448\n",
            "[0/10][20/300]\tLoss_H: 0.0022 Loss_R: 0.0071 Loss_sum: 0.0076 \tdatatime: 0.0017 \tbatchtime: 9.8438\n",
            "[0/10][30/300]\tLoss_H: 0.0033 Loss_R: 0.0034 Loss_sum: 0.0058 \tdatatime: 0.0017 \tbatchtime: 10.0178\n",
            "[0/10][40/300]\tLoss_H: 0.0035 Loss_R: 0.0052 Loss_sum: 0.0074 \tdatatime: 0.0017 \tbatchtime: 9.9356\n",
            "[0/10][50/300]\tLoss_H: 0.0030 Loss_R: 0.0070 Loss_sum: 0.0082 \tdatatime: 0.0019 \tbatchtime: 9.8813\n",
            "[0/10][60/300]\tLoss_H: 0.0026 Loss_R: 0.0028 Loss_sum: 0.0047 \tdatatime: 0.0016 \tbatchtime: 10.0900\n",
            "[0/10][70/300]\tLoss_H: 0.0025 Loss_R: 0.0052 Loss_sum: 0.0064 \tdatatime: 0.0016 \tbatchtime: 9.8194\n",
            "[0/10][80/300]\tLoss_H: 0.0024 Loss_R: 0.0040 Loss_sum: 0.0054 \tdatatime: 0.0016 \tbatchtime: 9.9806\n",
            "[0/10][90/300]\tLoss_H: 0.0027 Loss_R: 0.0056 Loss_sum: 0.0069 \tdatatime: 0.0017 \tbatchtime: 9.9079\n",
            "[0/10][100/300]\tLoss_H: 0.0026 Loss_R: 0.0036 Loss_sum: 0.0053 \tdatatime: 0.0018 \tbatchtime: 10.1546\n",
            "[0/10][110/300]\tLoss_H: 0.0025 Loss_R: 0.0040 Loss_sum: 0.0056 \tdatatime: 0.0017 \tbatchtime: 9.8463\n",
            "[0/10][120/300]\tLoss_H: 0.0027 Loss_R: 0.0048 Loss_sum: 0.0063 \tdatatime: 0.0016 \tbatchtime: 9.8822\n",
            "[0/10][130/300]\tLoss_H: 0.0034 Loss_R: 0.0066 Loss_sum: 0.0083 \tdatatime: 0.0017 \tbatchtime: 9.8863\n",
            "[0/10][140/300]\tLoss_H: 0.0026 Loss_R: 0.0081 Loss_sum: 0.0087 \tdatatime: 0.0016 \tbatchtime: 9.8223\n",
            "[0/10][150/300]\tLoss_H: 0.0026 Loss_R: 0.0045 Loss_sum: 0.0059 \tdatatime: 0.0023 \tbatchtime: 9.7787\n",
            "[0/10][160/300]\tLoss_H: 0.0030 Loss_R: 0.0029 Loss_sum: 0.0051 \tdatatime: 0.0015 \tbatchtime: 9.7513\n",
            "[0/10][170/300]\tLoss_H: 0.0033 Loss_R: 0.0064 Loss_sum: 0.0080 \tdatatime: 0.0017 \tbatchtime: 9.8165\n",
            "[0/10][180/300]\tLoss_H: 0.0033 Loss_R: 0.0038 Loss_sum: 0.0061 \tdatatime: 0.0017 \tbatchtime: 9.8631\n",
            "[0/10][190/300]\tLoss_H: 0.0024 Loss_R: 0.0049 Loss_sum: 0.0061 \tdatatime: 0.0017 \tbatchtime: 9.7281\n",
            "[0/10][200/300]\tLoss_H: 0.0031 Loss_R: 0.0047 Loss_sum: 0.0067 \tdatatime: 0.0016 \tbatchtime: 9.8960\n",
            "[0/10][210/300]\tLoss_H: 0.0038 Loss_R: 0.0029 Loss_sum: 0.0060 \tdatatime: 0.0015 \tbatchtime: 9.7980\n",
            "[0/10][220/300]\tLoss_H: 0.0022 Loss_R: 0.0022 Loss_sum: 0.0039 \tdatatime: 0.0016 \tbatchtime: 9.7703\n",
            "[0/10][230/300]\tLoss_H: 0.0050 Loss_R: 0.0050 Loss_sum: 0.0087 \tdatatime: 0.0015 \tbatchtime: 9.7496\n",
            "[0/10][240/300]\tLoss_H: 0.0033 Loss_R: 0.0042 Loss_sum: 0.0064 \tdatatime: 0.0017 \tbatchtime: 9.9467\n",
            "[0/10][250/300]\tLoss_H: 0.0024 Loss_R: 0.0045 Loss_sum: 0.0058 \tdatatime: 0.0016 \tbatchtime: 9.7287\n",
            "[0/10][260/300]\tLoss_H: 0.0029 Loss_R: 0.0041 Loss_sum: 0.0060 \tdatatime: 0.0018 \tbatchtime: 9.9679\n",
            "[0/10][270/300]\tLoss_H: 0.0036 Loss_R: 0.0073 Loss_sum: 0.0090 \tdatatime: 0.0018 \tbatchtime: 9.9343\n",
            "[0/10][280/300]\tLoss_H: 0.0025 Loss_R: 0.0041 Loss_sum: 0.0056 \tdatatime: 0.0024 \tbatchtime: 9.7040\n",
            "[0/10][290/300]\tLoss_H: 0.0026 Loss_R: 0.0056 Loss_sum: 0.0067 \tdatatime: 0.0018 \tbatchtime: 9.8731\n",
            "one epoch time is 3082.5891======================================================================\n",
            "epoch learning rate: optimizerH_lr = 0.00100000      optimizerR_lr = 0.00100000\n",
            "epoch_Hloss=0.003044\tepoch_Rloss=0.004706\tepoch_sumLoss=0.006574\n",
            "#################################################### validation begin ########################################################\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:21: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:22: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:29: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "validation[0] val_Hloss = 0.002636\t val_Rloss = 0.007840\t val_Sumloss = 0.008516\t validation time=131.23\n",
            "#################################################### validation end ########################################################\n",
            "[1/10][0/300]\tLoss_H: 0.0023 Loss_R: 0.0038 Loss_sum: 0.0052 \tdatatime: 1.6570 \tbatchtime: 20.8794\n",
            "[1/10][10/300]\tLoss_H: 0.0029 Loss_R: 0.0047 Loss_sum: 0.0065 \tdatatime: 0.0069 \tbatchtime: 9.8779\n",
            "[1/10][20/300]\tLoss_H: 0.0037 Loss_R: 0.0038 Loss_sum: 0.0066 \tdatatime: 0.0016 \tbatchtime: 10.1335\n",
            "[1/10][30/300]\tLoss_H: 0.0029 Loss_R: 0.0059 Loss_sum: 0.0073 \tdatatime: 0.0016 \tbatchtime: 9.8816\n",
            "[1/10][40/300]\tLoss_H: 0.0033 Loss_R: 0.0029 Loss_sum: 0.0054 \tdatatime: 0.0021 \tbatchtime: 9.9025\n",
            "[1/10][50/300]\tLoss_H: 0.0030 Loss_R: 0.0039 Loss_sum: 0.0059 \tdatatime: 0.0015 \tbatchtime: 9.8876\n",
            "[1/10][60/300]\tLoss_H: 0.0032 Loss_R: 0.0044 Loss_sum: 0.0065 \tdatatime: 0.0017 \tbatchtime: 9.8575\n",
            "[1/10][70/300]\tLoss_H: 0.0028 Loss_R: 0.0093 Loss_sum: 0.0097 \tdatatime: 0.0019 \tbatchtime: 9.8281\n",
            "[1/10][80/300]\tLoss_H: 0.0026 Loss_R: 0.0040 Loss_sum: 0.0056 \tdatatime: 0.0017 \tbatchtime: 9.8718\n",
            "[1/10][90/300]\tLoss_H: 0.0034 Loss_R: 0.0062 Loss_sum: 0.0080 \tdatatime: 0.0026 \tbatchtime: 10.0903\n",
            "[1/10][100/300]\tLoss_H: 0.0031 Loss_R: 0.0047 Loss_sum: 0.0066 \tdatatime: 0.0018 \tbatchtime: 9.9155\n",
            "[1/10][110/300]\tLoss_H: 0.0030 Loss_R: 0.0031 Loss_sum: 0.0053 \tdatatime: 0.0015 \tbatchtime: 9.8077\n",
            "[1/10][120/300]\tLoss_H: 0.0031 Loss_R: 0.0031 Loss_sum: 0.0054 \tdatatime: 0.0015 \tbatchtime: 9.8956\n",
            "[1/10][130/300]\tLoss_H: 0.0029 Loss_R: 0.0052 Loss_sum: 0.0067 \tdatatime: 0.0017 \tbatchtime: 9.9878\n",
            "[1/10][140/300]\tLoss_H: 0.0031 Loss_R: 0.0038 Loss_sum: 0.0059 \tdatatime: 0.0015 \tbatchtime: 9.9736\n",
            "[1/10][150/300]\tLoss_H: 0.0022 Loss_R: 0.0064 Loss_sum: 0.0070 \tdatatime: 0.0015 \tbatchtime: 10.0006\n",
            "[1/10][160/300]\tLoss_H: 0.0030 Loss_R: 0.0042 Loss_sum: 0.0062 \tdatatime: 0.0017 \tbatchtime: 10.1437\n",
            "[1/10][170/300]\tLoss_H: 0.0026 Loss_R: 0.0031 Loss_sum: 0.0049 \tdatatime: 0.0016 \tbatchtime: 10.1850\n",
            "[1/10][180/300]\tLoss_H: 0.0029 Loss_R: 0.0038 Loss_sum: 0.0058 \tdatatime: 0.0017 \tbatchtime: 10.4072\n",
            "[1/10][190/300]\tLoss_H: 0.0027 Loss_R: 0.0052 Loss_sum: 0.0066 \tdatatime: 0.0022 \tbatchtime: 10.5773\n",
            "[1/10][200/300]\tLoss_H: 0.0027 Loss_R: 0.0031 Loss_sum: 0.0051 \tdatatime: 0.0017 \tbatchtime: 9.9655\n",
            "[1/10][210/300]\tLoss_H: 0.0033 Loss_R: 0.0022 Loss_sum: 0.0050 \tdatatime: 0.0017 \tbatchtime: 9.9149\n",
            "[1/10][220/300]\tLoss_H: 0.0025 Loss_R: 0.0036 Loss_sum: 0.0052 \tdatatime: 0.0017 \tbatchtime: 9.9154\n",
            "[1/10][230/300]\tLoss_H: 0.0028 Loss_R: 0.0036 Loss_sum: 0.0055 \tdatatime: 0.0015 \tbatchtime: 9.8645\n",
            "[1/10][240/300]\tLoss_H: 0.0025 Loss_R: 0.0049 Loss_sum: 0.0062 \tdatatime: 0.0024 \tbatchtime: 9.9588\n",
            "[1/10][250/300]\tLoss_H: 0.0018 Loss_R: 0.0027 Loss_sum: 0.0038 \tdatatime: 0.0018 \tbatchtime: 9.8365\n",
            "[1/10][260/300]\tLoss_H: 0.0033 Loss_R: 0.0047 Loss_sum: 0.0068 \tdatatime: 0.0017 \tbatchtime: 9.9520\n",
            "[1/10][270/300]\tLoss_H: 0.0017 Loss_R: 0.0057 Loss_sum: 0.0060 \tdatatime: 0.0017 \tbatchtime: 10.1312\n",
            "[1/10][280/300]\tLoss_H: 0.0027 Loss_R: 0.0033 Loss_sum: 0.0051 \tdatatime: 0.0017 \tbatchtime: 9.9119\n",
            "[1/10][290/300]\tLoss_H: 0.0026 Loss_R: 0.0047 Loss_sum: 0.0061 \tdatatime: 0.0016 \tbatchtime: 9.8445\n",
            "one epoch time is 3124.4624======================================================================\n",
            "epoch learning rate: optimizerH_lr = 0.00100000      optimizerR_lr = 0.00100000\n",
            "epoch_Hloss=0.002882\tepoch_Rloss=0.003989\tepoch_sumLoss=0.005874\n",
            "#################################################### validation begin ########################################################\n",
            "validation[1] val_Hloss = 0.002497\t val_Rloss = 0.007261\t val_Sumloss = 0.007943\t validation time=135.31\n",
            "#################################################### validation end ########################################################\n",
            "[2/10][0/300]\tLoss_H: 0.0028 Loss_R: 0.0041 Loss_sum: 0.0059 \tdatatime: 1.8215 \tbatchtime: 20.8059\n",
            "[2/10][10/300]\tLoss_H: 0.0026 Loss_R: 0.0072 Loss_sum: 0.0079 \tdatatime: 0.0002 \tbatchtime: 9.9872\n",
            "[2/10][20/300]\tLoss_H: 0.0025 Loss_R: 0.0037 Loss_sum: 0.0053 \tdatatime: 0.0018 \tbatchtime: 10.0614\n",
            "[2/10][30/300]\tLoss_H: 0.0028 Loss_R: 0.0051 Loss_sum: 0.0066 \tdatatime: 0.0017 \tbatchtime: 10.1523\n",
            "[2/10][40/300]\tLoss_H: 0.0022 Loss_R: 0.0032 Loss_sum: 0.0045 \tdatatime: 0.0016 \tbatchtime: 10.0200\n",
            "[2/10][50/300]\tLoss_H: 0.0041 Loss_R: 0.0037 Loss_sum: 0.0068 \tdatatime: 0.0019 \tbatchtime: 10.1110\n",
            "[2/10][60/300]\tLoss_H: 0.0022 Loss_R: 0.0042 Loss_sum: 0.0053 \tdatatime: 0.0017 \tbatchtime: 10.0408\n",
            "[2/10][70/300]\tLoss_H: 0.0026 Loss_R: 0.0030 Loss_sum: 0.0048 \tdatatime: 0.0024 \tbatchtime: 10.1545\n",
            "[2/10][80/300]\tLoss_H: 0.0041 Loss_R: 0.0046 Loss_sum: 0.0076 \tdatatime: 0.0018 \tbatchtime: 10.0747\n",
            "[2/10][90/300]\tLoss_H: 0.0049 Loss_R: 0.0046 Loss_sum: 0.0083 \tdatatime: 0.0017 \tbatchtime: 10.1396\n",
            "[2/10][100/300]\tLoss_H: 0.0023 Loss_R: 0.0029 Loss_sum: 0.0044 \tdatatime: 0.0017 \tbatchtime: 10.0407\n",
            "[2/10][110/300]\tLoss_H: 0.0030 Loss_R: 0.0047 Loss_sum: 0.0066 \tdatatime: 0.0015 \tbatchtime: 10.0668\n",
            "[2/10][120/300]\tLoss_H: 0.0019 Loss_R: 0.0024 Loss_sum: 0.0037 \tdatatime: 0.0016 \tbatchtime: 9.9835\n",
            "[2/10][130/300]\tLoss_H: 0.0028 Loss_R: 0.0033 Loss_sum: 0.0053 \tdatatime: 0.0018 \tbatchtime: 10.1439\n",
            "[2/10][140/300]\tLoss_H: 0.0028 Loss_R: 0.0037 Loss_sum: 0.0056 \tdatatime: 0.0018 \tbatchtime: 10.0329\n",
            "[2/10][150/300]\tLoss_H: 0.0024 Loss_R: 0.0022 Loss_sum: 0.0040 \tdatatime: 0.0016 \tbatchtime: 10.1187\n",
            "[2/10][160/300]\tLoss_H: 0.0034 Loss_R: 0.0051 Loss_sum: 0.0072 \tdatatime: 0.0016 \tbatchtime: 10.0574\n",
            "[2/10][170/300]\tLoss_H: 0.0027 Loss_R: 0.0026 Loss_sum: 0.0047 \tdatatime: 0.0015 \tbatchtime: 10.0255\n",
            "[2/10][180/300]\tLoss_H: 0.0030 Loss_R: 0.0067 Loss_sum: 0.0080 \tdatatime: 0.0017 \tbatchtime: 9.9431\n",
            "[2/10][190/300]\tLoss_H: 0.0015 Loss_R: 0.0042 Loss_sum: 0.0047 \tdatatime: 0.0017 \tbatchtime: 10.0673\n",
            "[2/10][200/300]\tLoss_H: 0.0019 Loss_R: 0.0030 Loss_sum: 0.0041 \tdatatime: 0.0018 \tbatchtime: 10.2556\n",
            "[2/10][210/300]\tLoss_H: 0.0040 Loss_R: 0.0026 Loss_sum: 0.0060 \tdatatime: 0.0018 \tbatchtime: 10.1718\n",
            "[2/10][220/300]\tLoss_H: 0.0019 Loss_R: 0.0029 Loss_sum: 0.0040 \tdatatime: 0.0018 \tbatchtime: 9.9864\n",
            "[2/10][230/300]\tLoss_H: 0.0020 Loss_R: 0.0027 Loss_sum: 0.0040 \tdatatime: 0.0018 \tbatchtime: 10.3293\n",
            "[2/10][240/300]\tLoss_H: 0.0029 Loss_R: 0.0039 Loss_sum: 0.0058 \tdatatime: 0.0017 \tbatchtime: 9.9397\n",
            "[2/10][250/300]\tLoss_H: 0.0034 Loss_R: 0.0024 Loss_sum: 0.0052 \tdatatime: 0.0017 \tbatchtime: 9.9731\n",
            "[2/10][260/300]\tLoss_H: 0.0028 Loss_R: 0.0025 Loss_sum: 0.0048 \tdatatime: 0.0017 \tbatchtime: 10.1185\n",
            "[2/10][270/300]\tLoss_H: 0.0041 Loss_R: 0.0037 Loss_sum: 0.0069 \tdatatime: 0.0017 \tbatchtime: 10.4008\n",
            "[2/10][280/300]\tLoss_H: 0.0033 Loss_R: 0.0031 Loss_sum: 0.0056 \tdatatime: 0.0017 \tbatchtime: 10.0204\n",
            "[2/10][290/300]\tLoss_H: 0.0029 Loss_R: 0.0039 Loss_sum: 0.0059 \tdatatime: 0.0015 \tbatchtime: 10.1833\n",
            "one epoch time is 3154.1024======================================================================\n",
            "epoch learning rate: optimizerH_lr = 0.00100000      optimizerR_lr = 0.00100000\n",
            "epoch_Hloss=0.002675\tepoch_Rloss=0.003601\tepoch_sumLoss=0.005375\n",
            "#################################################### validation begin ########################################################\n",
            "validation[2] val_Hloss = 0.002418\t val_Rloss = 0.013976\t val_Sumloss = 0.012900\t validation time=134.44\n",
            "#################################################### validation end ########################################################\n",
            "[3/10][0/300]\tLoss_H: 0.0021 Loss_R: 0.0022 Loss_sum: 0.0038 \tdatatime: 1.8171 \tbatchtime: 20.2841\n",
            "[3/10][10/300]\tLoss_H: 0.0022 Loss_R: 0.0035 Loss_sum: 0.0048 \tdatatime: 0.0031 \tbatchtime: 9.9825\n",
            "[3/10][20/300]\tLoss_H: 0.0024 Loss_R: 0.0043 Loss_sum: 0.0056 \tdatatime: 0.0023 \tbatchtime: 10.1274\n",
            "[3/10][30/300]\tLoss_H: 0.0037 Loss_R: 0.0034 Loss_sum: 0.0063 \tdatatime: 0.0018 \tbatchtime: 10.0523\n",
            "[3/10][40/300]\tLoss_H: 0.0025 Loss_R: 0.0028 Loss_sum: 0.0046 \tdatatime: 0.0017 \tbatchtime: 10.0337\n",
            "[3/10][50/300]\tLoss_H: 0.0024 Loss_R: 0.0023 Loss_sum: 0.0041 \tdatatime: 0.0018 \tbatchtime: 9.9563\n",
            "[3/10][60/300]\tLoss_H: 0.0027 Loss_R: 0.0039 Loss_sum: 0.0056 \tdatatime: 0.0016 \tbatchtime: 10.1028\n",
            "[3/10][70/300]\tLoss_H: 0.0019 Loss_R: 0.0022 Loss_sum: 0.0035 \tdatatime: 0.0018 \tbatchtime: 10.1518\n",
            "[3/10][80/300]\tLoss_H: 0.0020 Loss_R: 0.0029 Loss_sum: 0.0042 \tdatatime: 0.0019 \tbatchtime: 10.0359\n",
            "[3/10][90/300]\tLoss_H: 0.0019 Loss_R: 0.0038 Loss_sum: 0.0047 \tdatatime: 0.0018 \tbatchtime: 10.1203\n",
            "[3/10][100/300]\tLoss_H: 0.0022 Loss_R: 0.0031 Loss_sum: 0.0045 \tdatatime: 0.0017 \tbatchtime: 10.2592\n",
            "[3/10][110/300]\tLoss_H: 0.0027 Loss_R: 0.0024 Loss_sum: 0.0045 \tdatatime: 0.0018 \tbatchtime: 10.1018\n",
            "[3/10][120/300]\tLoss_H: 0.0025 Loss_R: 0.0061 Loss_sum: 0.0070 \tdatatime: 0.0016 \tbatchtime: 10.0840\n",
            "[3/10][130/300]\tLoss_H: 0.0028 Loss_R: 0.0024 Loss_sum: 0.0046 \tdatatime: 0.0018 \tbatchtime: 9.9477\n",
            "[3/10][140/300]\tLoss_H: 0.0031 Loss_R: 0.0042 Loss_sum: 0.0063 \tdatatime: 0.0018 \tbatchtime: 10.8178\n",
            "[3/10][150/300]\tLoss_H: 0.0025 Loss_R: 0.0023 Loss_sum: 0.0042 \tdatatime: 0.0019 \tbatchtime: 9.9943\n",
            "[3/10][160/300]\tLoss_H: 0.0027 Loss_R: 0.0036 Loss_sum: 0.0054 \tdatatime: 0.0018 \tbatchtime: 10.1295\n",
            "[3/10][170/300]\tLoss_H: 0.0022 Loss_R: 0.0057 Loss_sum: 0.0065 \tdatatime: 0.0018 \tbatchtime: 10.0256\n",
            "[3/10][180/300]\tLoss_H: 0.0022 Loss_R: 0.0031 Loss_sum: 0.0045 \tdatatime: 0.0023 \tbatchtime: 10.0281\n",
            "[3/10][190/300]\tLoss_H: 0.0035 Loss_R: 0.0057 Loss_sum: 0.0077 \tdatatime: 0.0017 \tbatchtime: 10.0132\n",
            "[3/10][200/300]\tLoss_H: 0.0034 Loss_R: 0.0023 Loss_sum: 0.0051 \tdatatime: 0.0019 \tbatchtime: 10.0179\n",
            "[3/10][210/300]\tLoss_H: 0.0022 Loss_R: 0.0022 Loss_sum: 0.0039 \tdatatime: 0.0016 \tbatchtime: 10.0099\n",
            "[3/10][220/300]\tLoss_H: 0.0029 Loss_R: 0.0034 Loss_sum: 0.0054 \tdatatime: 0.0017 \tbatchtime: 10.0587\n",
            "[3/10][230/300]\tLoss_H: 0.0024 Loss_R: 0.0039 Loss_sum: 0.0054 \tdatatime: 0.0017 \tbatchtime: 10.0341\n",
            "[3/10][240/300]\tLoss_H: 0.0025 Loss_R: 0.0026 Loss_sum: 0.0044 \tdatatime: 0.0019 \tbatchtime: 10.0912\n",
            "[3/10][250/300]\tLoss_H: 0.0022 Loss_R: 0.0038 Loss_sum: 0.0051 \tdatatime: 0.0017 \tbatchtime: 10.0434\n",
            "[3/10][260/300]\tLoss_H: 0.0020 Loss_R: 0.0033 Loss_sum: 0.0045 \tdatatime: 0.0024 \tbatchtime: 10.0618\n",
            "[3/10][270/300]\tLoss_H: 0.0025 Loss_R: 0.0023 Loss_sum: 0.0043 \tdatatime: 0.0021 \tbatchtime: 9.9965\n",
            "[3/10][280/300]\tLoss_H: 0.0023 Loss_R: 0.0022 Loss_sum: 0.0040 \tdatatime: 0.0017 \tbatchtime: 10.0442\n",
            "[3/10][290/300]\tLoss_H: 0.0027 Loss_R: 0.0032 Loss_sum: 0.0051 \tdatatime: 0.0019 \tbatchtime: 9.9887\n",
            "one epoch time is 3152.1441======================================================================\n",
            "epoch learning rate: optimizerH_lr = 0.00100000      optimizerR_lr = 0.00100000\n",
            "epoch_Hloss=0.002573\tepoch_Rloss=0.003221\tepoch_sumLoss=0.004989\n",
            "#################################################### validation begin ########################################################\n",
            "validation[3] val_Hloss = 0.002095\t val_Rloss = 0.004277\t val_Sumloss = 0.005303\t validation time=141.43\n",
            "#################################################### validation end ########################################################\n",
            "[4/10][0/300]\tLoss_H: 0.0026 Loss_R: 0.0027 Loss_sum: 0.0046 \tdatatime: 1.8647 \tbatchtime: 20.6861\n",
            "[4/10][10/300]\tLoss_H: 0.0019 Loss_R: 0.0024 Loss_sum: 0.0037 \tdatatime: 0.0002 \tbatchtime: 9.9018\n",
            "[4/10][20/300]\tLoss_H: 0.0018 Loss_R: 0.0034 Loss_sum: 0.0044 \tdatatime: 0.0017 \tbatchtime: 10.0101\n",
            "[4/10][30/300]\tLoss_H: 0.0035 Loss_R: 0.0037 Loss_sum: 0.0063 \tdatatime: 0.0016 \tbatchtime: 10.0888\n",
            "[4/10][40/300]\tLoss_H: 0.0028 Loss_R: 0.0025 Loss_sum: 0.0047 \tdatatime: 0.0016 \tbatchtime: 10.0424\n",
            "[4/10][50/300]\tLoss_H: 0.0024 Loss_R: 0.0028 Loss_sum: 0.0045 \tdatatime: 0.0018 \tbatchtime: 10.2114\n",
            "[4/10][60/300]\tLoss_H: 0.0024 Loss_R: 0.0020 Loss_sum: 0.0039 \tdatatime: 0.0020 \tbatchtime: 10.0090\n",
            "[4/10][70/300]\tLoss_H: 0.0046 Loss_R: 0.0028 Loss_sum: 0.0067 \tdatatime: 0.0018 \tbatchtime: 10.0104\n",
            "[4/10][80/300]\tLoss_H: 0.0031 Loss_R: 0.0054 Loss_sum: 0.0072 \tdatatime: 0.0016 \tbatchtime: 10.3098\n",
            "[4/10][90/300]\tLoss_H: 0.0015 Loss_R: 0.0046 Loss_sum: 0.0050 \tdatatime: 0.0017 \tbatchtime: 10.1513\n",
            "[4/10][100/300]\tLoss_H: 0.0026 Loss_R: 0.0038 Loss_sum: 0.0055 \tdatatime: 0.0016 \tbatchtime: 10.0174\n",
            "[4/10][110/300]\tLoss_H: 0.0020 Loss_R: 0.0030 Loss_sum: 0.0043 \tdatatime: 0.0022 \tbatchtime: 10.0141\n",
            "[4/10][120/300]\tLoss_H: 0.0021 Loss_R: 0.0028 Loss_sum: 0.0042 \tdatatime: 0.0016 \tbatchtime: 10.0021\n",
            "[4/10][130/300]\tLoss_H: 0.0021 Loss_R: 0.0034 Loss_sum: 0.0046 \tdatatime: 0.0018 \tbatchtime: 10.0141\n",
            "[4/10][140/300]\tLoss_H: 0.0018 Loss_R: 0.0033 Loss_sum: 0.0043 \tdatatime: 0.0021 \tbatchtime: 10.0718\n",
            "[4/10][150/300]\tLoss_H: 0.0020 Loss_R: 0.0037 Loss_sum: 0.0048 \tdatatime: 0.0017 \tbatchtime: 9.9878\n",
            "[4/10][160/300]\tLoss_H: 0.0019 Loss_R: 0.0020 Loss_sum: 0.0034 \tdatatime: 0.0018 \tbatchtime: 9.9246\n",
            "[4/10][170/300]\tLoss_H: 0.0023 Loss_R: 0.0020 Loss_sum: 0.0038 \tdatatime: 0.0017 \tbatchtime: 10.0731\n",
            "[4/10][180/300]\tLoss_H: 0.0026 Loss_R: 0.0034 Loss_sum: 0.0052 \tdatatime: 0.0016 \tbatchtime: 10.1306\n",
            "[4/10][190/300]\tLoss_H: 0.0021 Loss_R: 0.0022 Loss_sum: 0.0037 \tdatatime: 0.0016 \tbatchtime: 9.9269\n",
            "[4/10][200/300]\tLoss_H: 0.0020 Loss_R: 0.0034 Loss_sum: 0.0045 \tdatatime: 0.0016 \tbatchtime: 9.9863\n",
            "[4/10][210/300]\tLoss_H: 0.0025 Loss_R: 0.0049 Loss_sum: 0.0062 \tdatatime: 0.0018 \tbatchtime: 10.1201\n",
            "[4/10][220/300]\tLoss_H: 0.0017 Loss_R: 0.0019 Loss_sum: 0.0031 \tdatatime: 0.0018 \tbatchtime: 10.0213\n",
            "[4/10][230/300]\tLoss_H: 0.0021 Loss_R: 0.0018 Loss_sum: 0.0034 \tdatatime: 0.0016 \tbatchtime: 9.9620\n",
            "[4/10][240/300]\tLoss_H: 0.0016 Loss_R: 0.0031 Loss_sum: 0.0039 \tdatatime: 0.0021 \tbatchtime: 10.1039\n",
            "[4/10][250/300]\tLoss_H: 0.0032 Loss_R: 0.0031 Loss_sum: 0.0056 \tdatatime: 0.0018 \tbatchtime: 10.0025\n",
            "[4/10][260/300]\tLoss_H: 0.0025 Loss_R: 0.0040 Loss_sum: 0.0055 \tdatatime: 0.0017 \tbatchtime: 9.9373\n",
            "[4/10][270/300]\tLoss_H: 0.0028 Loss_R: 0.0033 Loss_sum: 0.0053 \tdatatime: 0.0016 \tbatchtime: 9.9952\n",
            "[4/10][280/300]\tLoss_H: 0.0026 Loss_R: 0.0030 Loss_sum: 0.0049 \tdatatime: 0.0018 \tbatchtime: 9.9608\n",
            "[4/10][290/300]\tLoss_H: 0.0028 Loss_R: 0.0030 Loss_sum: 0.0051 \tdatatime: 0.0021 \tbatchtime: 9.9254\n",
            "one epoch time is 3145.5909======================================================================\n",
            "epoch learning rate: optimizerH_lr = 0.00100000      optimizerR_lr = 0.00100000\n",
            "epoch_Hloss=0.002452\tepoch_Rloss=0.003069\tepoch_sumLoss=0.004754\n",
            "#################################################### validation begin ########################################################\n",
            "validation[4] val_Hloss = 0.002088\t val_Rloss = 0.003490\t val_Sumloss = 0.004705\t validation time=136.16\n",
            "#################################################### validation end ########################################################\n",
            "[5/10][0/300]\tLoss_H: 0.0020 Loss_R: 0.0029 Loss_sum: 0.0041 \tdatatime: 1.8560 \tbatchtime: 20.4204\n",
            "[5/10][10/300]\tLoss_H: 0.0017 Loss_R: 0.0023 Loss_sum: 0.0034 \tdatatime: 0.0002 \tbatchtime: 10.2154\n",
            "[5/10][20/300]\tLoss_H: 0.0028 Loss_R: 0.0040 Loss_sum: 0.0058 \tdatatime: 0.0019 \tbatchtime: 9.9994\n",
            "[5/10][30/300]\tLoss_H: 0.0023 Loss_R: 0.0047 Loss_sum: 0.0058 \tdatatime: 0.0022 \tbatchtime: 10.0773\n",
            "[5/10][40/300]\tLoss_H: 0.0016 Loss_R: 0.0018 Loss_sum: 0.0030 \tdatatime: 0.0019 \tbatchtime: 10.0045\n",
            "[5/10][50/300]\tLoss_H: 0.0027 Loss_R: 0.0025 Loss_sum: 0.0046 \tdatatime: 0.0017 \tbatchtime: 10.0506\n",
            "[5/10][60/300]\tLoss_H: 0.0026 Loss_R: 0.0031 Loss_sum: 0.0049 \tdatatime: 0.0017 \tbatchtime: 9.9863\n",
            "[5/10][70/300]\tLoss_H: 0.0025 Loss_R: 0.0028 Loss_sum: 0.0045 \tdatatime: 0.0015 \tbatchtime: 10.0977\n",
            "[5/10][80/300]\tLoss_H: 0.0040 Loss_R: 0.0037 Loss_sum: 0.0067 \tdatatime: 0.0017 \tbatchtime: 10.0745\n",
            "[5/10][90/300]\tLoss_H: 0.0026 Loss_R: 0.0026 Loss_sum: 0.0045 \tdatatime: 0.0017 \tbatchtime: 10.0311\n",
            "[5/10][100/300]\tLoss_H: 0.0019 Loss_R: 0.0020 Loss_sum: 0.0035 \tdatatime: 0.0017 \tbatchtime: 9.9055\n",
            "[5/10][110/300]\tLoss_H: 0.0023 Loss_R: 0.0028 Loss_sum: 0.0044 \tdatatime: 0.0017 \tbatchtime: 10.0007\n",
            "[5/10][120/300]\tLoss_H: 0.0018 Loss_R: 0.0038 Loss_sum: 0.0047 \tdatatime: 0.0018 \tbatchtime: 10.2252\n",
            "[5/10][130/300]\tLoss_H: 0.0029 Loss_R: 0.0024 Loss_sum: 0.0047 \tdatatime: 0.0020 \tbatchtime: 10.2074\n",
            "[5/10][140/300]\tLoss_H: 0.0030 Loss_R: 0.0019 Loss_sum: 0.0044 \tdatatime: 0.0019 \tbatchtime: 10.2089\n",
            "[5/10][150/300]\tLoss_H: 0.0027 Loss_R: 0.0028 Loss_sum: 0.0048 \tdatatime: 0.0017 \tbatchtime: 9.9836\n",
            "[5/10][160/300]\tLoss_H: 0.0021 Loss_R: 0.0020 Loss_sum: 0.0037 \tdatatime: 0.0018 \tbatchtime: 10.0257\n",
            "[5/10][170/300]\tLoss_H: 0.0024 Loss_R: 0.0038 Loss_sum: 0.0052 \tdatatime: 0.0018 \tbatchtime: 10.0059\n",
            "[5/10][180/300]\tLoss_H: 0.0018 Loss_R: 0.0021 Loss_sum: 0.0034 \tdatatime: 0.0018 \tbatchtime: 10.1652\n",
            "[5/10][190/300]\tLoss_H: 0.0023 Loss_R: 0.0036 Loss_sum: 0.0050 \tdatatime: 0.0018 \tbatchtime: 10.0474\n",
            "[5/10][200/300]\tLoss_H: 0.0022 Loss_R: 0.0029 Loss_sum: 0.0044 \tdatatime: 0.0021 \tbatchtime: 10.0818\n",
            "[5/10][210/300]\tLoss_H: 0.0015 Loss_R: 0.0024 Loss_sum: 0.0033 \tdatatime: 0.0023 \tbatchtime: 10.0633\n",
            "[5/10][220/300]\tLoss_H: 0.0026 Loss_R: 0.0024 Loss_sum: 0.0044 \tdatatime: 0.0017 \tbatchtime: 10.1137\n",
            "[5/10][230/300]\tLoss_H: 0.0025 Loss_R: 0.0036 Loss_sum: 0.0053 \tdatatime: 0.0017 \tbatchtime: 10.0417\n",
            "[5/10][240/300]\tLoss_H: 0.0018 Loss_R: 0.0023 Loss_sum: 0.0035 \tdatatime: 0.0016 \tbatchtime: 10.0498\n",
            "[5/10][250/300]\tLoss_H: 0.0021 Loss_R: 0.0049 Loss_sum: 0.0057 \tdatatime: 0.0016 \tbatchtime: 9.9487\n",
            "[5/10][260/300]\tLoss_H: 0.0020 Loss_R: 0.0018 Loss_sum: 0.0033 \tdatatime: 0.0019 \tbatchtime: 10.1388\n",
            "[5/10][270/300]\tLoss_H: 0.0024 Loss_R: 0.0034 Loss_sum: 0.0049 \tdatatime: 0.0023 \tbatchtime: 10.1845\n",
            "[5/10][280/300]\tLoss_H: 0.0020 Loss_R: 0.0028 Loss_sum: 0.0041 \tdatatime: 0.0021 \tbatchtime: 10.1081\n",
            "[5/10][290/300]\tLoss_H: 0.0024 Loss_R: 0.0025 Loss_sum: 0.0043 \tdatatime: 0.0015 \tbatchtime: 9.9072\n",
            "one epoch time is 3145.7262======================================================================\n",
            "epoch learning rate: optimizerH_lr = 0.00100000      optimizerR_lr = 0.00100000\n",
            "epoch_Hloss=0.002366\tepoch_Rloss=0.002805\tepoch_sumLoss=0.004470\n",
            "#################################################### validation begin ########################################################\n",
            "validation[5] val_Hloss = 0.001985\t val_Rloss = 0.018565\t val_Sumloss = 0.015909\t validation time=136.74\n",
            "#################################################### validation end ########################################################\n",
            "[6/10][0/300]\tLoss_H: 0.0025 Loss_R: 0.0082 Loss_sum: 0.0086 \tdatatime: 1.8529 \tbatchtime: 20.5627\n",
            "[6/10][10/300]\tLoss_H: 0.0020 Loss_R: 0.0028 Loss_sum: 0.0040 \tdatatime: 0.0002 \tbatchtime: 10.0016\n",
            "[6/10][20/300]\tLoss_H: 0.0025 Loss_R: 0.0019 Loss_sum: 0.0040 \tdatatime: 0.0018 \tbatchtime: 10.0124\n",
            "[6/10][30/300]\tLoss_H: 0.0024 Loss_R: 0.0024 Loss_sum: 0.0041 \tdatatime: 0.0017 \tbatchtime: 12.0110\n",
            "[6/10][40/300]\tLoss_H: 0.0017 Loss_R: 0.0023 Loss_sum: 0.0034 \tdatatime: 0.0025 \tbatchtime: 10.1041\n",
            "[6/10][50/300]\tLoss_H: 0.0019 Loss_R: 0.0022 Loss_sum: 0.0035 \tdatatime: 0.0016 \tbatchtime: 10.1418\n",
            "[6/10][60/300]\tLoss_H: 0.0022 Loss_R: 0.0021 Loss_sum: 0.0038 \tdatatime: 0.0017 \tbatchtime: 10.1253\n",
            "[6/10][70/300]\tLoss_H: 0.0024 Loss_R: 0.0061 Loss_sum: 0.0069 \tdatatime: 0.0016 \tbatchtime: 10.0244\n",
            "[6/10][80/300]\tLoss_H: 0.0024 Loss_R: 0.0025 Loss_sum: 0.0043 \tdatatime: 0.0016 \tbatchtime: 10.1392\n",
            "[6/10][90/300]\tLoss_H: 0.0024 Loss_R: 0.0034 Loss_sum: 0.0050 \tdatatime: 0.0017 \tbatchtime: 10.0087\n",
            "[6/10][100/300]\tLoss_H: 0.0029 Loss_R: 0.0022 Loss_sum: 0.0045 \tdatatime: 0.0018 \tbatchtime: 9.9313\n",
            "[6/10][110/300]\tLoss_H: 0.0018 Loss_R: 0.0018 Loss_sum: 0.0031 \tdatatime: 0.0023 \tbatchtime: 9.9705\n",
            "[6/10][120/300]\tLoss_H: 0.0017 Loss_R: 0.0020 Loss_sum: 0.0032 \tdatatime: 0.0018 \tbatchtime: 9.9799\n",
            "[6/10][130/300]\tLoss_H: 0.0023 Loss_R: 0.0024 Loss_sum: 0.0041 \tdatatime: 0.0018 \tbatchtime: 9.9964\n",
            "[6/10][140/300]\tLoss_H: 0.0031 Loss_R: 0.0026 Loss_sum: 0.0050 \tdatatime: 0.0018 \tbatchtime: 10.1007\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhGuBq8jinEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SSIM_Image_Steganography.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eereenah/deep-steganography/blob/master/SSIM_Image_Steganography.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud98xN3LiRbR",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ocZ7PT68hHcY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import functools\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import shutil\n",
        "import socket\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "import torchvision.utils as vutils\n",
        "#from tensorboardX import SummaryWriter\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_8PdKP_HgQv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP-i5m3FiQYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "from PIL import Image, ImageOps, ImageEnhance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzYJBhGOiYBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    import accimage\n",
        "except ImportError:\n",
        "    accimage = None\n",
        "import numpy as np\n",
        "import numbers\n",
        "import types\n",
        "import collections\n",
        "import warnings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtQXwwxHaul5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! pip install pytorch-msssim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8rwLitOa-tR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NV3AaFtx-Y9F",
        "colab_type": "text"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiiTms2-8E4x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget images.cocodataset.org/zips/val2017.zip\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj3DsJX18rLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip -qq *.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZWnAO4LbHnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm *.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8CszwA5e9stZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! wget images.cocodataset.org/zips/test2017.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtV6MhU8bNDL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! unzip -qq *.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIFu7aJEbPsT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm *.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhdYejDEl_QG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir 'val2017/cover'\n",
        "! mkdir 'val2017/secret'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD-A1c7Fm0CC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! find 'val2017/' -maxdepth 1 -type f -printf \".\" | wc -c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1E2ptSJoaa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls 'val2017/'* | head -480 | xargs -I{} mv {} 'val2017/cover/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xd9K0c6Cw3cJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls 'val2017/'* | head -480| xargs -I{} mv {} 'val2017/secret/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M47zTd_Nbcbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir 'train2017'\n",
        "! mkdir 'train2017/cover'\n",
        "! mkdir 'train2017/secret'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G-wpcMdbikg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! find 'test2017/' -maxdepth 1 -type f -printf \".\" | wc -c"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0ye6oHtbm_P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls 'test2017/'* | head -4800 | xargs -I{} mv {} 'train2017/cover/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjScNIQFbqST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls 'test2017/'* | head -4800 | xargs -I{} mv {} 'train2017/secret/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsiwGz67byRJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! mkdir 'test2017/cover'\n",
        "! mkdir 'test2017/secret'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBHqBPonY8ca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls 'test2017/'* | head -480 | xargs -I{} mv {} 'test2017/cover/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkB2xLQKZBHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls 'test2017/'* | head -4800 | xargs -I{} mv {} 'test2017/secret/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t60A1y5Ww4Dq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzisbWqoxFIq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4dqDq5qh2Qw",
        "colab_type": "text"
      },
      "source": [
        "### Hiding Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phYnFeB9hoLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UnetGenerator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, num_downs, ngf=64,\n",
        "                 norm_layer=nn.BatchNorm2d, use_dropout=False, output_function=nn.Sigmoid):\n",
        "        super(UnetGenerator, self).__init__()\n",
        "        # construct unet structure\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=None, norm_layer=norm_layer, innermost=True)\n",
        "        for i in range(num_downs - 5):\n",
        "            unet_block = UnetSkipConnectionBlock(ngf * 8, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer, use_dropout=use_dropout)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 4, ngf * 8, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf * 2, ngf * 4, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(ngf, ngf * 2, input_nc=None, submodule=unet_block, norm_layer=norm_layer)\n",
        "        unet_block = UnetSkipConnectionBlock(output_nc, ngf, input_nc=input_nc, submodule=unet_block, outermost=True, norm_layer=norm_layer, output_function=output_function)\n",
        "\n",
        "        self.model = unet_block\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.model(input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YePxcnYKhw8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UnetSkipConnectionBlock(nn.Module):\n",
        "    def __init__(self, outer_nc, inner_nc, input_nc=None,submodule=None, outermost=False, innermost=False, norm_layer=nn.BatchNorm2d, use_dropout=False, output_function=nn.Sigmoid):\n",
        "        super(UnetSkipConnectionBlock, self).__init__()\n",
        "        self.outermost = outermost\n",
        "        if type(norm_layer) == functools.partial:\n",
        "            use_bias = norm_layer.func == nn.InstanceNorm2d\n",
        "        else:\n",
        "            use_bias = norm_layer == nn.InstanceNorm2d\n",
        "        if input_nc is None:\n",
        "            input_nc = outer_nc\n",
        "        downconv = nn.Conv2d(input_nc, inner_nc, kernel_size=4,\n",
        "                             stride=2, padding=1, bias=use_bias)\n",
        "        downrelu = nn.LeakyReLU(0.2, True)\n",
        "        downnorm = norm_layer(inner_nc)\n",
        "        uprelu = nn.ReLU(True)\n",
        "        upnorm = norm_layer(outer_nc)\n",
        "\n",
        "        if outermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1)\n",
        "            down = [downconv]\n",
        "            if output_function == nn.Tanh:\n",
        "                up = [uprelu, upconv, nn.Tanh()]\n",
        "            else:\n",
        "                up = [uprelu, upconv, nn.Sigmoid()]\n",
        "            model = down + [submodule] + up\n",
        "        elif innermost:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1, bias=use_bias)\n",
        "            down = [downrelu, downconv]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "            model = down + up\n",
        "        else:\n",
        "            upconv = nn.ConvTranspose2d(inner_nc * 2, outer_nc,\n",
        "                                        kernel_size=4, stride=2,\n",
        "                                        padding=1, bias=use_bias)\n",
        "            down = [downrelu, downconv, downnorm]\n",
        "            up = [uprelu, upconv, upnorm]\n",
        "\n",
        "            if use_dropout:\n",
        "                model = down + [submodule] + up + [nn.Dropout(0.5)]\n",
        "            else:\n",
        "                model = down + [submodule] + up\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.outermost:\n",
        "            return self.model(x)\n",
        "        else:\n",
        "            return torch.cat([x, self.model(x)], 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjO163cUh-BY",
        "colab_type": "text"
      },
      "source": [
        "### Revealing Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxZ-MJlDhzsw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RevealNet(nn.Module):\n",
        "    def __init__(self, nc=3, nhf=64, output_function=nn.Sigmoid):\n",
        "        super(RevealNet, self).__init__()\n",
        "        # input is (3) x 256 x 256\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(nc, nhf, 3, 1, 1),\n",
        "            nn.BatchNorm2d(nhf),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(nhf, nhf * 2, 3, 1, 1),\n",
        "            nn.BatchNorm2d(nhf*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(nhf * 2, nhf * 4, 3, 1, 1),\n",
        "            nn.BatchNorm2d(nhf*4),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(nhf * 4, nhf * 2, 3, 1, 1),\n",
        "            nn.BatchNorm2d(nhf*2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(nhf * 2, nhf, 3, 1, 1),\n",
        "            nn.BatchNorm2d(nhf),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(nhf, nc, 3, 1, 1),\n",
        "            output_function()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        output=self.main(input)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgGI49wcj3Sf",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka9xm2VgU_-u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt_beta = 0.75\n",
        "opt_niter = 10\n",
        "opt_logFreq = 10\n",
        "opt_saveFreq = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvvYvP4Sl6-x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        m.weight.data.normal_(0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.normal_(1.0, 0.02)\n",
        "        m.bias.data.fill_(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQk6it-5mPNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(train_loader, epoch, Hnet, Rnet, criterion):\n",
        "    batch_time = AverageMeter()\n",
        "    data_time = AverageMeter()\n",
        "    Hlosses = AverageMeter()  # record loss of H-net\n",
        "    Rlosses = AverageMeter()  # record loss of R-net\n",
        "    SumLosses = AverageMeter()  # record Hloss + β*Rloss\n",
        "\n",
        "    # switch to train mode\n",
        "    Hnet.train()\n",
        "    Rnet.train()\n",
        "\n",
        "    start_time = time.time()\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        data_time.update(time.time() - start_time)\n",
        "\n",
        "        Hnet.zero_grad()\n",
        "        Rnet.zero_grad()\n",
        "\n",
        "        all_pics = data[0]  # allpics contains cover images and secret images\n",
        "        this_batch_size = int(all_pics.size()[0] / 2)  # get true batch size of this step \n",
        "\n",
        "        # first half of images will become cover images, the rest are treated as secret images\n",
        "        cover_img = all_pics[0:this_batch_size, :, :, :]  # batchsize,3,256,256\n",
        "        secret_img = all_pics[this_batch_size:this_batch_size * 2, :, :, :]\n",
        "\n",
        "        # concat cover images and secret images as input of H-net\n",
        "        concat_img = torch.cat([cover_img, secret_img], dim=1)\n",
        "\n",
        "        concat_imgv = Variable(concat_img)\n",
        "        cover_imgv = Variable(cover_img)\n",
        "\n",
        "        container_img = Hnet(concat_imgv)  # put concat_image into H-net and get container image\n",
        "        #errH = criterion(container_img, cover_imgv)  # loss between cover and container\n",
        "        errH = -torch.mean(ssim(container_img, cover_imgv, data_range=255, size_average=False, nonnegative_ssim=True))\n",
        "        Hlosses.update(errH.data.item(), this_batch_size)\n",
        "\n",
        "        rev_secret_img = Rnet(container_img)  # put concatenated image into R-net and get revealed secret image\n",
        "        secret_imgv = Variable(secret_img)\n",
        "        #errR = criterion(rev_secret_img, secret_imgv)  # loss between secret image and revealed secret image\n",
        "        errR = -torch.mean(ssim(rev_secret_img, secret_imgv, data_range=255, size_average=False, nonnegative_ssim=True))\n",
        "        Rlosses.update(errR.data.item(), this_batch_size)\n",
        "\n",
        "        betaerrR_secret = opt_beta * errR\n",
        "        err_sum = errH + betaerrR_secret\n",
        "        SumLosses.update(err_sum.data.item(), this_batch_size)\n",
        "\n",
        "        err_sum.backward()\n",
        "\n",
        "        optimizerH.step()\n",
        "        optimizerR.step()\n",
        "\n",
        "        batch_time.update(time.time() - start_time)\n",
        "        start_time = time.time()\n",
        "\n",
        "        log = '[%d/%d][%d/%d]\\tLoss_H: %.4f Loss_R: %.4f Loss_sum: %.4f \\tdatatime: %.4f \\tbatchtime: %.4f' % (\n",
        "            epoch, opt_niter, i, len(train_loader),\n",
        "            Hlosses.val, Rlosses.val, SumLosses.val, data_time.val, batch_time.val)\n",
        "\n",
        "        if i % opt_logFreq == 0:\n",
        "            print(log)\n",
        "            with open('drive/My Drive/Steganography/Models/UNet-Pytorch/Checkpoints/Checkpoints_SSIM_4800_10_pt2/Parameters.txt', 'a') as f: \n",
        "                f.write(log + \"\\n\")\n",
        "\n",
        "        if epoch % 1 == 0 and i % opt_saveFreq == 0:\n",
        "            save_result_pic(this_batch_size, cover_img, container_img.data, secret_img, rev_secret_img.data, epoch, i,\n",
        "                            'drive/My Drive/Steganography/Models/UNet-Pytorch/Train/Train_SSIM_4800_10_pt2/')\n",
        "\n",
        "\n",
        "    # epcoh log\n",
        "    epoch_log = \"one epoch time is %.4f======================================================================\" % (\n",
        "        batch_time.sum) + \"\\n\"\n",
        "    epoch_log = epoch_log + \"epoch learning rate: optimizerH_lr = %.8f      optimizerR_lr = %.8f\" % (\n",
        "        optimizerH.param_groups[0]['lr'], optimizerR.param_groups[0]['lr']) + \"\\n\"\n",
        "    epoch_log = epoch_log + \"epoch_Hloss=%.6f\\tepoch_Rloss=%.6f\\tepoch_sumLoss=%.6f\" % (\n",
        "        Hlosses.avg, Rlosses.avg, SumLosses.avg)\n",
        "    print(epoch_log)\n",
        "    with open('drive/My Drive/Steganography/Models/UNet-Pytorch/Checkpoints/Checkpoints_SSIM_4800_10_pt2/Parameters.txt', 'a') as f: \n",
        "                f.write(epoch_log + \"\\n\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qjgQnhCmVlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validation(val_loader, epoch, Hnet, Rnet, criterion):\n",
        "    print(\n",
        "        \"#################################################### validation begin ########################################################\")\n",
        "    start_time = time.time()\n",
        "    Hnet.eval()\n",
        "    Rnet.eval()\n",
        "    Hlosses = AverageMeter()\n",
        "    Rlosses = AverageMeter()\n",
        "    for i, data in enumerate(val_loader, 0):\n",
        "        Hnet.zero_grad()\n",
        "        Rnet.zero_grad()\n",
        "        all_pics = data[0]\n",
        "        this_batch_size = int(all_pics.size()[0] / 2)\n",
        "\n",
        "        cover_img = all_pics[0:this_batch_size, :, :, :]\n",
        "        secret_img = all_pics[this_batch_size:this_batch_size * 2, :, :, :]\n",
        "\n",
        "        concat_img = torch.cat([cover_img, secret_img], dim=1)\n",
        "\n",
        "\n",
        "        concat_imgv = Variable(concat_img, volatile=True)\n",
        "        cover_imgv = Variable(cover_img, volatile=True)\n",
        "\n",
        "        container_img = Hnet(concat_imgv)\n",
        "        #errH = criterion(container_img, cover_imgv)\n",
        "        errH = -torch.mean(ssim(container_img, cover_imgv, data_range=255, size_average=False, nonnegative_ssim=True))\n",
        "        Hlosses.update(errH.data.item(), this_batch_size)\n",
        "\n",
        "        rev_secret_img = Rnet(container_img)\n",
        "        secret_imgv = Variable(secret_img, volatile=True)\n",
        "        #errR = criterion(rev_secret_img, secret_imgv)\n",
        "        errR = -torch.mean(ssim(rev_secret_img, secret_imgv, data_range=255, size_average=False, nonnegative_ssim=True))\n",
        "        Rlosses.update(errR.data.item(), this_batch_size)\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            save_result_pic(this_batch_size, cover_img, container_img.data, secret_img, rev_secret_img.data, epoch, i,\n",
        "                            'drive/My Drive/Steganography/Models/UNet-Pytorch/Validation/Validation_SSIM_4800_10_p2/')\n",
        "\n",
        "\n",
        "    val_hloss = Hlosses.avg\n",
        "    val_rloss = Rlosses.avg\n",
        "    val_sumloss = val_hloss + opt_beta * val_rloss\n",
        "\n",
        "    val_time = time.time() - start_time\n",
        "    val_log = \"validation[%d] val_Hloss = %.6f\\t val_Rloss = %.6f\\t val_Sumloss = %.6f\\t validation time=%.2f\" % (\n",
        "        epoch, val_hloss, val_rloss, val_sumloss, val_time)\n",
        "    #print_log(val_log, logPath)\n",
        "    print(val_log)\n",
        "    with open('drive/My Drive/Steganography/Models/UNet-Pytorch/Checkpoints/Checkpoints_SSIM_4800_10_p2/Parameters.txt', 'a') as f: \n",
        "                f.write(val_log + \"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "    print(\n",
        "        \"#################################################### validation end ########################################################\")\n",
        "    return val_hloss, val_rloss, val_sumloss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMeBCZpVmar9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(test_loader, epoch, Hnet, Rnet, criterion):\n",
        "    print(\n",
        "        \"#################################################### test begin ########################################################\")\n",
        "    start_time = time.time()\n",
        "    Hnet.eval()\n",
        "    Rnet.eval()\n",
        "    Hlosses = AverageMeter()  # record the Hloss in one epoch\n",
        "    Rlosses = AverageMeter()  # record the Rloss in one epoch\n",
        "    for i, data in enumerate(test_loader, 0):\n",
        "        Hnet.zero_grad()\n",
        "        Rnet.zero_grad()\n",
        "        all_pics = data[0]  # allpics contains cover images and secret images\n",
        "        this_batch_size = int(all_pics.size()[0] / 2)  # get true batch size of this step \n",
        "\n",
        "        # first half of images will become cover images, the rest are treated as secret images\n",
        "        cover_img = all_pics[0:this_batch_size, :, :, :]  # batchSize,3,256,256\n",
        "        secret_img = all_pics[this_batch_size:this_batch_size * 2, :, :, :]\n",
        "\n",
        "        # concat cover and original secret to get the concat_img with 6 channels\n",
        "        concat_img = torch.cat([cover_img, secret_img], dim=1)\n",
        "\n",
        "\n",
        "        concat_imgv = Variable(concat_img, volatile=True)  # concat_img as input of Hiding net\n",
        "        cover_imgv = Variable(cover_img, volatile=True)  # cover_imgv as label of Hiding net\n",
        "\n",
        "        container_img = Hnet(concat_imgv)  # take concat_img as input of H-net and get the container_img\n",
        "        #errH = criterion(container_img, cover_imgv)  # H-net reconstructed error\n",
        "        errH = -torch.mean(ssim(container_img, cover_imgv, data_range=255, size_average=False, nonnegative_ssim=True))\n",
        "        Hlosses.update(errH.data.item(), this_batch_size)\n",
        "\n",
        "        rev_secret_img = Rnet(container_img)  # containerImg as input of R-net and get \"rev_secret_img\"\n",
        "        secret_imgv = Variable(secret_img, volatile=True)  # secret_imgv as label of R-net\n",
        "        #errR = criterion(rev_secret_img, secret_imgv)  # R-net reconstructed error\n",
        "        errR = -torch.mean(ssim(rev_secret_img, secret_imgv, data_range=255, size_average=False, nonnegative_ssim=True))\n",
        "        Rlosses.update(errR.data.item(), this_batch_size)\n",
        "        save_result_pic(this_batch_size, cover_img, container_img.data, secret_img, rev_secret_img.data, epoch, i,\n",
        "                            'drive/My Drive/Steganography/Models/UNet-Pytorch/Test/Test_SSIM_4800_10/')\n",
        "        # save_result_pic(this_batch_size, cover_img, container_img.data, secret_img, rev_secret_img.data, epoch, i,\n",
        "        #                 opt.testPics)\n",
        "\n",
        "    val_hloss = Hlosses.avg\n",
        "    val_rloss = Rlosses.avg\n",
        "    val_sumloss = val_hloss + opt_beta * val_rloss\n",
        "\n",
        "    val_time = time.time() - start_time\n",
        "    val_log = \"validation[%d] val_Hloss = %.6f\\t val_Rloss = %.6f\\t val_Sumloss = %.6f\\t validation time=%.2f\" % (\n",
        "        epoch, val_hloss, val_rloss, val_sumloss, val_time)\n",
        "    #print_log(val_log, logPath)\n",
        "    print(val_log)\n",
        "    with open('drive/My Drive/Steganography/Models/UNet-Pytorch/Checkpoints/Checkpoints_SSIM_4800_10_pt2/Parameters.txt', 'a') as f: \n",
        "                f.write(val_log + \"\\n\")\n",
        "\n",
        "\n",
        "    print(\n",
        "        \"#################################################### test end ########################################################\")\n",
        "    return val_hloss, val_rloss, val_sumloss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSvZbCZYme3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_result_pic(this_batch_size, originalLabelv, ContainerImg, secretLabelv, RevSecImg, epoch, i, save_path):\n",
        "      originalFrames = originalLabelv.resize_(this_batch_size, 3, 256, 256)\n",
        "      containerFrames = ContainerImg.resize_(this_batch_size, 3, 256, 256)\n",
        "      secretFrames = secretLabelv.resize_(this_batch_size, 3, 256, 256)\n",
        "      revSecFrames = RevSecImg.resize_(this_batch_size, 3, 256, 256)\n",
        "\n",
        "      showContainer = torch.cat([originalFrames, containerFrames], 0)\n",
        "      showReveal = torch.cat([secretFrames, revSecFrames], 0)\n",
        "      resultImg = torch.cat([showContainer, showReveal], 0)\n",
        "      resultImgName = '%s/ResultPics_epoch%03d_batch%04d.png' % (save_path, epoch, i)\n",
        "      vutils.save_image(resultImg, resultImgName, nrow=this_batch_size, padding=1, normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kw7hp6m8mhSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPNlq18CmHwH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    #global parameters\n",
        "    global opt, optimizerH, optimizerR, writer, logPath, schedulerH, schedulerR, val_loader, smallestLoss\n",
        "\n",
        "    Hnet = UnetGenerator(input_nc=6, output_nc=3, num_downs=7, output_function=nn.Sigmoid)\n",
        "    Hnet.apply(weights_init)\n",
        "\n",
        "    Rnet = RevealNet(output_function=nn.Sigmoid)\n",
        "    Rnet.apply(weights_init)\n",
        "\n",
        "    Hnet.load_state_dict(torch.load(\"drive/My Drive/Steganography/Models/UNet-Pytorch/Checkpoints/Checkpoints_SSIM_4800_10/netH_epoch_4,sumloss=-1.749747,Hloss=-0.999883.pth\"))\n",
        "    Rnet.load_state_dict(torch.load(\"drive/My Drive/Steganography/Models/UNet-Pytorch/Checkpoints/Checkpoints_SSIM_4800_10/netR_epoch_4,sumloss=-1.749747,Rloss=-0.999818.pth\"))\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizerH = optim.Adam(Hnet.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
        "    schedulerH = ReduceLROnPlateau(optimizerH, mode='min', factor=0.2, patience=5, verbose=True)\n",
        "\n",
        "    optimizerR = optim.Adam(Rnet.parameters(), lr=0.001, betas=(0.5, 0.999))\n",
        "    schedulerR = ReduceLROnPlateau(optimizerR, mode='min', factor=0.2, patience=8, verbose=True)\n",
        "\n",
        "    train_list = torchvision.datasets.ImageFolder(\n",
        "        root='train2017/',\n",
        "        transform= torchvision.transforms.Compose([                                                      \n",
        "                                       torchvision.transforms.Resize([256, 256]),\n",
        "                                       torchvision.transforms.ToTensor()\n",
        "       ])\n",
        "    )\n",
        "\n",
        "\n",
        "    val_list = torchvision.datasets.ImageFolder(\n",
        "       root='val2017/',\n",
        "       transform= torchvision.transforms.Compose([                                                      \n",
        "                                       torchvision.transforms.Resize([256, 256]),\n",
        "                                       torchvision.transforms.ToTensor()\n",
        "       ])\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_list, batch_size=32,\n",
        "                              shuffle=True, num_workers=int(8))\n",
        "    val_loader = DataLoader(val_list, batch_size=2,\n",
        "                            shuffle=False, num_workers=int(8))\n",
        "    smallestLoss = 10000\n",
        "    for epoch in range(opt_niter):\n",
        "        #train\n",
        "        train(train_loader, epoch, Hnet=Hnet, Rnet=Rnet, criterion=criterion)\n",
        "\n",
        "        #validation\n",
        "        val_hloss, val_rloss, val_sumloss = validation(val_loader, epoch, Hnet=Hnet, Rnet=Rnet, criterion=criterion)\n",
        "\n",
        "        #learning rate\n",
        "        schedulerH.step(val_sumloss)\n",
        "        schedulerR.step(val_rloss)\n",
        "\n",
        "        #save model\n",
        "        if val_sumloss < globals()[\"smallestLoss\"]:\n",
        "            globals()[\"smallestLoss\"] = val_sumloss\n",
        "            torch.save(Hnet.state_dict(),\n",
        "                        '%s/netH_epoch_%d,sumloss=%.6f,Hloss=%.6f.pth' % (\n",
        "                            'drive/My Drive/Steganography/Models/UNet-Pytorch/Checkpoints/Checkpoints_SSIM_4800_10_pt2/', epoch, val_sumloss, val_hloss))\n",
        "            torch.save(Rnet.state_dict(),\n",
        "                        '%s/netR_epoch_%d,sumloss=%.6f,Rloss=%.6f.pth' % (\n",
        "                            'drive/My Drive/Steganography/Models/UNet-Pytorch/Checkpoints/Checkpoints_SSIM_4800_10_pt2/', epoch, val_sumloss, val_rloss))\n",
        "            \n",
        "    test_list = torchvision.datasets.ImageFolder(\n",
        "        root='test2017/',\n",
        "        transform= torchvision.transforms.Compose([                                                      \n",
        "                                        torchvision.transforms.Resize([256, 256]),\n",
        "                                        torchvision.transforms.ToTensor()\n",
        "        ])\n",
        "    )\n",
        "\n",
        "\n",
        "    test_loader = DataLoader(test_list, batch_size=32,\n",
        "                                  shuffle=False, num_workers=int(8))\n",
        "    test(test_loader, 0, Hnet=Hnet, Rnet=Rnet, criterion=criterion)\n",
        "    print(\"##################   test is completed, the result pic is saved in the ./training/yourcompuer+time/testPics/   ######################\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lHlW3x7mrEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhGuBq8jinEH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}